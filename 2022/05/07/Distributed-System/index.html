<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","version":"8.2.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="Introduction to Distributed System">
<meta property="og:type" content="article">
<meta property="og:title" content="Distributed System">
<meta property="og:url" content="http://example.com/2022/05/07/Distributed-System/index.html">
<meta property="og:site_name" content="YangOcean">
<meta property="og:description" content="Introduction to Distributed System">
<meta property="og:locale">
<meta property="og:image" content="https://www.researchgate.net/profile/Trevor-Mudge/publication/221147997/figure/fig1/AS:339672459956226@1457995633431/A-Typical-3-Tier-Server-Architecture-Tier-1-Web-Server-Tier-2-Application-Server-Tier.png">
<meta property="article:published_time" content="2022-05-07T17:46:26.000Z">
<meta property="article:modified_time" content="2022-05-11T23:57:14.376Z">
<meta property="article:author" content="Yangyang Cui">
<meta property="article:tag" content="Distributed System">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.researchgate.net/profile/Trevor-Mudge/publication/221147997/figure/fig1/AS:339672459956226@1457995633431/A-Typical-3-Tier-Server-Architecture-Tier-1-Web-Server-Tier-2-Application-Server-Tier.png">


<link rel="canonical" href="http://example.com/2022/05/07/Distributed-System/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>
<script type="text/javascript">
  // Wait for the page to load first
  var _prevOnload = window.onload;
  window.onload = function() {
      var switchLang = document.getElementsByClassName("menu-item menu-item-switch_lang")[0];
      switchLang.onclick = function() {
          var href = window.location.href;
          var indexOfEn = href.toLowerCase().indexOf('/en/');
          if(indexOfEn !== -1) {
              window.location.href = href.replace('/en/', '/');
          }
          else {
              window.location.href = href.replace(/(^http[s]?:\/\/[a-z0-9.]*[:?0-9]*\/)(.*)/i, '$1en/$2');
          }
          if(typeof(_prevOnload) === 'function') {
              _prevOnload();
          }
          return false;
      }
  }
</script><title>Distributed System | YangOcean</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YangOcean</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Second year in University of Manchetser</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-switch_lang"><a href="/en" rel="section"><i class="language fa-fw"></i>English</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Distributed-System"><span class="nav-number">1.</span> <span class="nav-text">Introduction to Distributed System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Evolution"><span class="nav-number">1.1.</span> <span class="nav-text">The Evolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples-of-Distributed-Systems"><span class="nav-number">1.2.</span> <span class="nav-text">Examples of Distributed Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Distributed-Systems"><span class="nav-number">1.3.</span> <span class="nav-text">Why Distributed Systems?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-a-Distributed-System"><span class="nav-number">1.4.</span> <span class="nav-text">What is a Distributed System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Characteristics-of-Distributed-System"><span class="nav-number">1.5.</span> <span class="nav-text">Characteristics of Distributed System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fallacies-%E8%B0%AC%E8%AF%AF-about-Distributed-System"><span class="nav-number">1.6.</span> <span class="nav-text">Fallacies 谬误 about Distributed System</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-System-Organization"><span class="nav-number">1.7.</span> <span class="nav-text">Distributed System Organization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Designed-for-Transparency"><span class="nav-number">1.8.</span> <span class="nav-text">Designed for Transparency</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-Concepts-Synchronization"><span class="nav-number">2.</span> <span class="nav-text">Basic Concepts: Synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Synchronization-is-needed-specially-when%E2%80%A6"><span class="nav-number">2.1.</span> <span class="nav-text">Synchronization is needed specially when…</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Synchronization-Challenges-in-Distributed-Systems"><span class="nav-number">2.2.</span> <span class="nav-text">Synchronization Challenges in Distributed Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Definition"><span class="nav-number">2.3.</span> <span class="nav-text">Definition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logical-Clocks"><span class="nav-number">2.4.</span> <span class="nav-text">Logical Clocks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-2-Stopping-Events-from-Happening-Simultaneously-When-Sharing-Resources"><span class="nav-number">2.5.</span> <span class="nav-text">Example 2: Stopping Events from Happening Simultaneously When Sharing Resources</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Centralised-Lock-Server-and-Mutual-Exclusion-Locks-Mutex"><span class="nav-number">2.6.</span> <span class="nav-text">Centralised Lock Server and Mutual Exclusion Locks (Mutex)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-Phase-Commit-Algorithm"><span class="nav-number">2.7.</span> <span class="nav-text">Two Phase Commit Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Electing-a-Coordinator-Node"><span class="nav-number">2.8.</span> <span class="nav-text">Electing a Coordinator Node</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clock-Synchronization"><span class="nav-number">2.9.</span> <span class="nav-text">Clock Synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cristian%E2%80%99s-Algorithm"><span class="nav-number">2.9.1.</span> <span class="nav-text">Cristian’s Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Berkeley-Algorithm"><span class="nav-number">2.9.2.</span> <span class="nav-text">The Berkeley Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deadlock"><span class="nav-number">3.</span> <span class="nav-text">Deadlock</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Example"><span class="nav-number">3.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dealing-with-a-Deadlock"><span class="nav-number">3.2.</span> <span class="nav-text">Dealing with a Deadlock</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Basic-Concepts-Naming-Schemes"><span class="nav-number">4.</span> <span class="nav-text">Introduction to Basic Concepts: Naming Schemes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-Internet-Naming"><span class="nav-number">4.1.</span> <span class="nav-text">Example: Internet Naming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Centralised-Naming-Approach"><span class="nav-number">4.2.</span> <span class="nav-text">Centralised Naming Approach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Free-for-All-Naming-Approach"><span class="nav-number">4.3.</span> <span class="nav-text">Free-for-All Naming Approach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-%E2%80%98Delegating-Naming-Responsibilities%E2%80%99-Approach"><span class="nav-number">4.4.</span> <span class="nav-text">The ‘Delegating Naming Responsibilities’ Approach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MAC-Addresses"><span class="nav-number">4.5.</span> <span class="nav-text">MAC Addresses</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IP-Addresses"><span class="nav-number">4.6.</span> <span class="nav-text">IP Addresses</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Domain-Names"><span class="nav-number">4.7.</span> <span class="nav-text">Domain Names</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Basic-Concepts-Protocols"><span class="nav-number">5.</span> <span class="nav-text">Introduction to Basic Concepts: Protocols</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Statelessness"><span class="nav-number">5.1.</span> <span class="nav-text">Statelessness</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Email"><span class="nav-number">5.2.</span> <span class="nav-text">Email</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Email-Associated-Protocols"><span class="nav-number">5.3.</span> <span class="nav-text">Email Associated Protocols</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sequential-vs-Multi-Processing-Concurrent-Parallel-and-Distributed-Computing"><span class="nav-number">6.</span> <span class="nav-text">Sequential vs. Multi Processing, Concurrent, Parallel and Distributed Computing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequential-Processing"><span class="nav-number">6.1.</span> <span class="nav-text">Sequential Processing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Processing-vs-Multi-Tasking"><span class="nav-number">6.2.</span> <span class="nav-text">Multi-Processing vs. Multi-Tasking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Processing-by-Forking"><span class="nav-number">6.3.</span> <span class="nav-text">Multi-Processing by Forking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Processing-by-Threading"><span class="nav-number">6.4.</span> <span class="nav-text">Multi-Processing by Threading</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Concurrent-Computing"><span class="nav-number">6.5.</span> <span class="nav-text">Concurrent Computing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parallel-Computing"><span class="nav-number">6.6.</span> <span class="nav-text">Parallel Computing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distributed-Computing"><span class="nav-number">6.7.</span> <span class="nav-text">Distributed Computing</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Architectures-of-Distributed-Systems"><span class="nav-number">7.</span> <span class="nav-text">Architectures of Distributed Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Software-Architectural-Styles"><span class="nav-number">7.1.</span> <span class="nav-text">Software Architectural Styles</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Centralised-System-Architectures-Examples"><span class="nav-number">7.2.</span> <span class="nav-text">Centralised System Architectures: Examples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Tiered-Client-Server-Architectures"><span class="nav-number">7.3.</span> <span class="nav-text">Multi-Tiered Client-Server Architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decentralised-System-Architectures-Peer-to-Peer-Systems"><span class="nav-number">7.4.</span> <span class="nav-text">Decentralised System Architectures: Peer-to-Peer Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Structured-Peer-to-Peer-Systems"><span class="nav-number">7.5.</span> <span class="nav-text">Structured Peer-to-Peer Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Structured-P2P-System-example"><span class="nav-number">7.6.</span> <span class="nav-text">Structured P2P System example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsrtuctured-Peer-to-Peer-Systems"><span class="nav-number">7.7.</span> <span class="nav-text">Unsrtuctured Peer-to-Peer Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-of-Searching-Methods"><span class="nav-number">7.8.</span> <span class="nav-text">Example of Searching Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flooding"><span class="nav-number">7.8.1.</span> <span class="nav-text">Flooding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random-Walks"><span class="nav-number">7.8.2.</span> <span class="nav-text">Random Walks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Making-Data-Search-more-Scalable-in-Unstructured-P-2-P-Systems"><span class="nav-number">7.9.</span> <span class="nav-text">Making Data Search more Scalable in Unstructured P-2-P Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hybrid-Architectures"><span class="nav-number">7.10.</span> <span class="nav-text">Hybrid Architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Collaborative-Distributed-Systems"><span class="nav-number">7.11.</span> <span class="nav-text">Collaborative Distributed Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Edge-Server-Systems"><span class="nav-number">7.12.</span> <span class="nav-text">Edge-Server Systems</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Inter-Process-Communication"><span class="nav-number">8.</span> <span class="nav-text">Inter-Process Communication</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Replication-of-Data-in-Distributed-Systems"><span class="nav-number">9.</span> <span class="nav-text">Replication of Data in Distributed Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Replication"><span class="nav-number">9.1.</span> <span class="nav-text">Data Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Increasing-Reliability-through-Replication"><span class="nav-number">9.2.</span> <span class="nav-text">Increasing Reliability through Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-Performance-through-Replication"><span class="nav-number">9.3.</span> <span class="nav-text">Improving Performance through Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Price-of-Replication"><span class="nav-number">9.4.</span> <span class="nav-text">The Price of Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replication-for-Performance"><span class="nav-number">9.5.</span> <span class="nav-text">Replication for Performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tight-Consistency-through-Synchronous-Replication"><span class="nav-number">9.6.</span> <span class="nav-text">Tight Consistency through Synchronous Replication</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Relaxing-Tight-Consistency"><span class="nav-number">9.7.</span> <span class="nav-text">Relaxing Tight Consistency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Consistency-Models-Assumptions"><span class="nav-number">9.8.</span> <span class="nav-text">Consistency Models - Assumptions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequential-Consistency-Model"><span class="nav-number">9.9.</span> <span class="nav-text">Sequential Consistency Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Causal-Consistency-Model"><span class="nav-number">9.10.</span> <span class="nav-text">Causal Consistency Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replica-Management"><span class="nav-number">9.11.</span> <span class="nav-text">Replica Management</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Security-in-Distributed-Systems"><span class="nav-number">10.</span> <span class="nav-text">Security in Distributed Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Relationship-between-Security-and-Dependability"><span class="nav-number">10.1.</span> <span class="nav-text">Relationship between Security and Dependability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Types-of-Security-Threats"><span class="nav-number">10.2.</span> <span class="nav-text">Types of Security Threats</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Security-Mechanisms"><span class="nav-number">10.3.</span> <span class="nav-text">Security Mechanisms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cryptography-%E5%AF%86%E7%A0%81%E5%AD%A6"><span class="nav-number">10.4.</span> <span class="nav-text">Cryptography 密码学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Security-in-Distributed-System-Recap"><span class="nav-number">10.5.</span> <span class="nav-text">Security in Distributed System-Recap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Secure-channels"><span class="nav-number">10.6.</span> <span class="nav-text">Secure channels</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-of-Autentication-Protocol"><span class="nav-number">10.7.</span> <span class="nav-text">Example of Autentication Protocol</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Secure-channels-Message-integrity-and-confidentiality"><span class="nav-number">10.8.</span> <span class="nav-text">Secure channels: Message integrity and confidentiality</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Message-integrity-via-Digital-Signature"><span class="nav-number">10.9.</span> <span class="nav-text">Message integrity via Digital Signature</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Service-Oriented-Architecture"><span class="nav-number">11.</span> <span class="nav-text">Service-Oriented Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-of-a-SOA-Application"><span class="nav-number">11.1.</span> <span class="nav-text">Example of a SOA Application</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Service-Composition"><span class="nav-number">11.2.</span> <span class="nav-text">Service Composition</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yangyang Cui"
      src="/uploads/selfie2.jpg">
  <p class="site-author-name" itemprop="name">Yangyang Cui</p>
  <div class="site-description" itemprop="description">share everything I want to share</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yangocean-sudo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yangocean-sudo" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/07/Distributed-System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/selfie2.jpg">
      <meta itemprop="name" content="Yangyang Cui">
      <meta itemprop="description" content="share everything I want to share">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YangOcean">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Distributed System
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-07 18:46:26" itemprop="dateCreated datePublished" datetime="2022-05-07T18:46:26+01:00">2022-05-07</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2022-05-12 00:57:14" itemprop="dateModified" datetime="2022-05-12T00:57:14+01:00">2022-05-12</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Introduction-to-Distributed-System"><a href="#Introduction-to-Distributed-System" class="headerlink" title="Introduction to Distributed System"></a>Introduction to Distributed System</h1><a id="more"></a>
<h2 id="The-Evolution"><a href="#The-Evolution" class="headerlink" title="The Evolution"></a>The Evolution</h2><ul>
<li>From 1945 to 1985<ul>
<li>Computers were large and expensive</li>
<li>It was not possible to connect computers, so they operated independently from one another.</li>
<li>From 1985<ul>
<li>Advances in technology began to change that situation, including:<ul>
<li>the development of powerful microprocessors,</li>
<li>the invention of high-speed computer networks, and</li>
<li>the miniaturization 小型化 of computer systems.</li>
</ul>
</li>
</ul>
</li>
<li>The result of these technologies is that it is, now, possible and relatively easy to put together <em>a computing system composed of large numbers of networked computers</em>, large or small.<ul>
<li>As these computers are generally dispersed分散的, they are said to form a <em>distributed system</em>.<h2 id="Examples-of-Distributed-Systems"><a href="#Examples-of-Distributed-Systems" class="headerlink" title="Examples of Distributed Systems"></a>Examples of Distributed Systems</h2></li>
</ul>
</li>
</ul>
</li>
<li>The Web over the Internet<ul>
<li>Documents, email, media, commerce, etc.</li>
</ul>
</li>
<li>Mobile telephony<ul>
<li>Calls, texts, location, sensing, etc.</li>
</ul>
</li>
<li>Electronic funds transfer <ul>
<li>Banking, credit cards, etc.</li>
</ul>
</li>
<li>Instant messaging</li>
<li>Video conferencing 会议</li>
<li>Home entertaining systems </li>
<li>Global positioning systems<h2 id="Why-Distributed-Systems"><a href="#Why-Distributed-Systems" class="headerlink" title="Why Distributed Systems?"></a>Why Distributed Systems?</h2></li>
<li>To share resources:<ul>
<li>One publisher, many beneficiaries!</li>
</ul>
</li>
<li>To bind customers and suppliers</li>
<li>To allow us to do things we could not otherwise do, due to<br>performance, scalability, reliability and availability issues. <ul>
<li>Performance issues: e.g.,if using 1 computer,it takes 60 minutes, I’ll use 100 computers and it will take 0.6 minutes! </li>
<li>Scalability issues: e.g.,if there’s 10 times more to do in th 0.6<br>minutes I have, I’ll use 1,000 computers, then, if it things quieten<br>down again, I’ll go back to using 100 computers!</li>
<li>Reliability and availability issues: e.g., if 1% of computers fails<br>every day, add an extra 1% to keep the 0.6 stable and on!<br>In other words:</li>
</ul>
</li>
<li>To make continuously-evolving不断发展, remote resources accessible for<br><em>sharing</em>.</li>
<li>To open proprietary 所有权 processes to external interaction in order to <em>foster cooperation</em> 促进合作.</li>
<li>To achieve better performance/cost ratios.</li>
<li>To scale effectively and efficiently if <em>demand需求 for resources</em> changes significantly.</li>
<li>To scale through modular, incremental <em>expansion and contraction</em>.</li>
<li>To achieve high levels of <em>reliability</em> and <em>availability</em>.<h2 id="What-is-a-Distributed-System"><a href="#What-is-a-Distributed-System" class="headerlink" title="What is a Distributed System"></a>What is a Distributed System</h2><blockquote>
<p>“A distributed system is a collection of autonomous 自主 computing elements that appears to its users as a single coherent相关 system”</p>
</blockquote>
</li>
<li>A computing element or a node, can be either a hardware<br>device or a software process<ul>
<li>Note that <em>a node can be anything from a high-performance mainframe computer to a small device in a sensor network</em>. Also, nodes can be interconnected in anyway.</li>
</ul>
</li>
<li>In a single coherent system, users (i.e., people or applications) believe they are dealing with a single system. <ul>
<li>Note that, for this to be possible, the autonomous nodes(each node will have its own notion of time) need to<br>collaborate. The way in which this collaboration is established represents the most fundamental property that distinguishes between different distributed systems.<h2 id="Characteristics-of-Distributed-System"><a href="#Characteristics-of-Distributed-System" class="headerlink" title="Characteristics of Distributed System"></a>Characteristics of Distributed System</h2></li>
</ul>
</li>
<li>Distributed systems can also vary in:<ul>
<li>Size(e.g.,from a few to millions of computers).</li>
<li>The way in which nodes are interconnected (e.g.,via a wired, wireless or a hybrid混合 (a combination of both) network)</li>
<li>The way in which node membership is handled(e.g.,by being open or closed group when allowing new nodes to join).</li>
</ul>
</li>
<li>Note that, although nodes can act independently from one another, they cannot ignore one another. Otherwise, there is no point in putting them to compose构成 the same system!</li>
<li>Nodes are programmed to achieve common goals, and they do this by <em>exchanging messages</em> with each other.</li>
<li>To appear to users as a single coherent system, <em>distribution transparency</em> is an important goal of distributed systems.</li>
<li>Computation is concurrent</li>
<li>There is no shared state<ul>
<li>No <em>single global clock</em> that all programs can agree to follow </li>
</ul>
</li>
<li>Failures occur may will not be noticeable</li>
<li>Communication events have non-negligible不可忽略 duration<ul>
<li>communication costs may be more siginificant than processing time</li>
</ul>
</li>
<li>Components may exchange data at variable rates可变速率.</li>
<li>Different components process data at different rates.<ul>
<li>Asynchrony异步 (i.e., non-aligned 不对齐的 timelines) is unavoidable.<h2 id="Fallacies-谬误-about-Distributed-System"><a href="#Fallacies-谬误-about-Distributed-System" class="headerlink" title="Fallacies 谬误 about Distributed System"></a>Fallacies 谬误 about Distributed System</h2><blockquote>
<p>“Distirbuted systems differ from traditional software because components are dispersed across a network. Not taking this dispersion into account during design time is what makes so many systems needlessly complex and results in flaws that need to be patched later on”.”<br>False assumptions that are commonly made when developing a distributed application:</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ol>
<li>The network is reliable</li>
</ol>
<ul>
<li>Various way of nodes linking (network topology)</li>
<li>DS is composed of different and varied (i.e., heterogeneous) resources (software and hardware)<ul>
<li>Example of <em>Lack of software reliability</em>: need for reliable message exchange between nodes (e.g., functionality for retrying messages, acknowledging承认 messages, verifying message integrity, etc.)</li>
<li>In essence, the result is a network of networks, each managed with local scope by a different group of administrators 每个网络由不同的管理员组在本地范围内管理</li>
<li>As a consequence, no (sufficiently足够的 complex to be useful) network of networks is reliable. 如果不足够复杂以至于有用的话，网络就不是可靠的</li>
</ul>
</li>
</ul>
<ol start="2">
<li>The network is secure</li>
</ol>
<ul>
<li>You may need to build security into your applications from Day 1.</li>
<li> As a result of security considerations, you might not be able to access networked resources, different user accounts may have different privileges.</li>
</ul>
<ol start="3">
<li>The network is homogeneous</li>
</ol>
<ul>
<li>Interoperability 互操作性 will be needed.</li>
<li>The use of standard technologies, such as XML (a W3C recommended general- purpose markup language) or JSON will be necessary.</li>
</ul>
<ol start="4">
<li>The topology does not change</li>
</ol>
<ul>
<li>Network topology is the way in which nodes of a distributed system are linked.</li>
<li>In the wild, servers may be added and removed often, clients (laptops, wireless ad hoc networks) are coming and going: the topology is changing constantly.</li>
<li>Do not rely on specific endpoints or routes.</li>
<li>Abstract from the physical structure of the network, by (using the most obvious example) using DNS names as opposed to IP addresses (which may vary) for referring to an endpoint.<ul>
<li>DNS (Internet Domain Name System) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It is commonly used to translate more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols.</li>
</ul>
</li>
</ul>
<ol start="5">
<li>Latency is zero</li>
</ol>
<ul>
<li>The minimum round-trip time between two points on earth is determined by the <em>maximum speed of information transmission</em>: the speed of light.<ul>
<li>At 300,000 km/sec, it will take at least 30msec to send a ping from Europe to the USA and back.<ul>
<li> <em>Ping</em> is a computer network administration software utility that measures the round-trip time for messages sent from the originating host to a destination computer that are echoed back to the source.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Bandwidth is infinite</li>
</ol>
<ul>
<li>Bandwidth is a measure of how much data it is possible to transfer over a period of time (may be measured in bits/second).</li>
<li>To avoid congestion拥塞 and increase connection throughput, the loss of data packets being transmitted from one point to another can be performed.数据包丢失的现象可能会随着增加链接吞吐量而发生<ul>
<li>Throughput is a measure of how much data is successfully transferred from source to destination within a given timeframe. In other words, it is used to measure how many packets arrive at their destinations successfully. It can be measured in bits/second. Throughput测量数据包传递的成功率</li>
<li>To avoid packet loss, we may want to use larger packet sizes.</li>
</ul>
</li>
</ul>
<ol start="7">
<li>Transport cost is zero</li>
</ol>
<ul>
<li>Information needs to be serialised (by marshalling编组) to get data onto the wire.<ul>
<li>Marshalling is the process of <em>transforming the memory representation</em> of an object to a <em>data format</em> suitable for storage or transmission.</li>
<li>The cost (in terms of money) for setting and running the network is not zero.</li>
</ul>
</li>
</ul>
<ol start="8">
<li>There is one administrator</li>
</ol>
<ul>
<li>Unless we refer to a small LAN, there will be <em>different administrators</em> associated with the network with different degrees of expertise.<h2 id="Distributed-System-Organization"><a href="#Distributed-System-Organization" class="headerlink" title="Distributed System Organization"></a>Distributed System Organization</h2></li>
<li> To assist the development of distributed applications, distributed systems are often organized to have a <em>separate layer of software</em> that is logically placed on top of the respective各自的 operating systems of the computers that are part of the system.</li>
<li>This separate layer is called middle layer and offers each application the same interface as well as the following:<ul>
<li>Facilities for inter-application communication.</li>
<li>Security services.</li>
<li>Support for transaction management.</li>
<li>Recovery from failures.</li>
<li>Support for Web services composition.</li>
<li>Etc.<h2 id="Designed-for-Transparency"><a href="#Designed-for-Transparency" class="headerlink" title="Designed for Transparency"></a>Designed for Transparency</h2>An object can be a resource or a process.</li>
</ul>
</li>
<li>Access - Hide differences in data representation and how an object is accessed</li>
<li>Location - Hide where an object is located.</li>
<li>Relocation - Hide that an object may be moved to another location while in use.</li>
<li>Migration - Hide that an object may move to <em>another</em> location.</li>
<li>Replication - Hide that an object is replicated.</li>
<li>Concurrency - Hide that an object may be shared by several independent users.</li>
<li>Failure - Hide the failure and recovery of an object.</li>
</ul>
<h1 id="Basic-Concepts-Synchronization"><a href="#Basic-Concepts-Synchronization" class="headerlink" title="Basic Concepts: Synchronization"></a>Basic Concepts: Synchronization</h1><ul>
<li>Synchronisation refers to <em>data synchronisation</em> and <em>process synchronisation</em></li>
<li><em>Data synchronisation</em>: about keeping multiple copies of a dataset in coherence with one another, given that the various copies are located in different nodes.<ul>
<li>E.g: Copy photos from mobile device to laptop</li>
</ul>
</li>
<li><em>Process synchronisation</em> : about multiple processes needing to act together to achieve a certain overall purpose.</li>
<li>Synchornisation requires <em>fast</em> and <em>reliable</em> communication between the process</li>
<li>None of the assumptions is safe, synchronisation behaviour is often challenging<h2 id="Synchronization-is-needed-specially-when…"><a href="#Synchronization-is-needed-specially-when…" class="headerlink" title="Synchronization is needed specially when…"></a>Synchronization is needed specially when…</h2></li>
<li>Multiple processes need to agree on the ordering of events, such as whether message m1 from process P was sent before or after message m2 from process Q.</li>
<li>Multiple processes try to simultaneously 同时 access a shared resource, such as a printer, and should, instead, cooperate in granting 授予 each other temporary临时的 exclusive独家的 access.<h2 id="Synchronization-Challenges-in-Distributed-Systems"><a href="#Synchronization-Challenges-in-Distributed-Systems" class="headerlink" title="Synchronization Challenges in Distributed Systems"></a>Synchronization Challenges in Distributed Systems</h2></li>
<li>Since nodes in a distributed system are connected via a network, and networks are not always reliable, coordination 协作 of actions that depend on communication over a network is quite challenging.<ul>
<li>For example, the same message sent to different nodes can take a <em>different time interval</em> to arrive at each node; if sent multiple times to the same node, it may take different times to arrive at the node, or sometimes it may not arrive at all.</li>
</ul>
</li>
<li>Sending a synchronisation message from one node to another could be considered, however, because there is no way of knowing exactly how long a message is going to take to arrive at its destination, this does not represent a good solution to this problem.</li>
<li>Another alternative选择 could be to <em>timestamp</em> a message as its sent out, but, as there is no way of knowing how long the message took to arrive at its destination, unless the recipient’s clock is in sync with the sender’s clock in the first place, which cannot be not guaranteed, as nodes are independent.</li>
<li>In fact, about two nodes with independent clocks, the only thing that is certain is that <em>a message will not arrive at its destination before it is sent.</em><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2></li>
<li> a -&gt; b is “event a happens before event b”.</li>
<li>If a and b are events in the same process, and a occurs before b, then a -&gt; b is true.</li>
<li>If a is the event of a message being sent by one process, and b is the event of the message being received by another process, then a -&gt; b is also true.</li>
<li>Message transmission takes a finite, nonzero amount of time</li>
<li>Happens-before (-&gt;) is a transitive relation, so if a -&gt; b and b -&gt; c, then a -&gt; c.</li>
<li>If two events, x and y, happen in different processes that <em>do not exchange messages</em> (not even indirectly via third parties), then x -&gt; y is not true, but neither is y -&gt; x. These events are said to be concurrent<h2 id="Logical-Clocks"><a href="#Logical-Clocks" class="headerlink" title="Logical Clocks"></a>Logical Clocks</h2></li>
<li>Logical clocks take advantage of the fact that <em>an implicit partial ordering of events can be obtained from the simple sending and receiving of messages between processes</em> in a distributed system. As such, they do not measure “real time” but, instead, <em>provide a distributed incremental pseudo time to events</em> in a distributed system.</li>
<li>A simple example is <em>Lamport’s logical clock</em>. It assumes that each processor i has a Logical Clock, LCi.<ol>
<li>When an event occurs on processor i, LCi is incremented by one.</li>
<li>When processor X sends a message to Y, it also sends its Logical Clock, LCx.</li>
<li>When Y receives the message, if its local Logical Clock is already in advance of the clock it has just received plus one timestep, it <em>keeps its current Logical Clock</em>, otherwise it sets its local Logical Clock to be the one it has just received plus one timestep, i.e.:<code>if LCy &lt; (LCx + 1): LCy = LCx + 1</code></li>
</ol>
</li>
<li>Notes on Lamport’s Logical Clock<ol>
<li>the receipt of a message forces the recipient to move its clock forward so that the “happened after” relationship is preserved 保存 at that point.</li>
<li>if a -&gt; b (a happens before b) then it is true that LCa &lt; LCb. But it is not necessarily true that just because LCa &lt; LCb then a -&gt; b. This means that <em>we cannot infer 推断 a causal 因果 relationship just by looking at timestamps</em>.</li>
<li>by using logical clocks, we can obtain a <em>basic partial ordering of events</em>, i.e., we can tell that one event happened after another one, but <em>we do not obtain a perfectly synchronised global time</em>.</li>
</ol>
</li>
</ul>
<h2 id="Example-2-Stopping-Events-from-Happening-Simultaneously-When-Sharing-Resources"><a href="#Example-2-Stopping-Events-from-Happening-Simultaneously-When-Sharing-Resources" class="headerlink" title="Example 2: Stopping Events from Happening Simultaneously When Sharing Resources"></a>Example 2: Stopping Events from Happening Simultaneously When Sharing Resources</h2><ul>
<li>Assume that there are <code>multiple distributed components collaborating to achieve one goal.</code></li>
<li>For example: the selling of a game ticket to a client. The involved components include a database of available tickets, at least two bank accounts (one for the ticket vendor, one for the person buying the ticket), and possibly other systems for validating addresses, and arranging for the ticket to be delivered electronically.</li>
<li>These systems need to <code>choreographed to act as one</code> for the purposes of selling a ticket, to avoid situations such as the following: (1) one ticket is selected by a client, which happens to be the last ticket. But, just before the client is able to pay for it, the ticket is purchased by another client who had also selected it; or (2) when paying for the ticket, the client confirms the payment transaction, and so the money leaves his account, but a system crash prevents the money from arriving at the seller’s account.</li>
<li>Possible solutions:<ul>
<li>Centralised Lock Server (and Mutual Exclusion Locks (mutex)) </li>
<li>Two Phase Commit Algorithm<h2 id="Centralised-Lock-Server-and-Mutual-Exclusion-Locks-Mutex"><a href="#Centralised-Lock-Server-and-Mutual-Exclusion-Locks-Mutex" class="headerlink" title="Centralised Lock Server and Mutual Exclusion Locks (Mutex)"></a>Centralised Lock Server and Mutual Exclusion Locks (Mutex)</h2></li>
</ul>
</li>
<li>Client:</li>
</ul>
<ol>
<li>Sends a request to the lock server for a mutex on a given resource.</li>
<li>When a reply comes back, it starts executing its critical process over the resource.</li>
<li>When its finished, it sends a message to release the mutex.</li>
</ol>
<ul>
<li>The Lock Server:</li>
</ul>
<ol>
<li>If the resource is available, it marks it as being used by the client and sends a reply to say that the lock has been granted. Otherwise, it puts the request in a queue.</li>
<li>When the mutex is released by the client, if another client wants the resource (i.e., is in the queue waiting), pass the mutex to them, otherwise mark it as being available.</li>
</ol>
<ul>
<li>This solution presents a basic limitation: a single point of failure (i.e. the central lock server).</li>
<li>Although the solution is able to protect sequences of events that need to be treated as ‘atomic’ (indivisible) operations, it cannot make sure that, if any part of the sequence of event fails, the state of the system remains unchanged.<h2 id="Two-Phase-Commit-Algorithm"><a href="#Two-Phase-Commit-Algorithm" class="headerlink" title="Two Phase Commit Algorithm"></a>Two Phase Commit Algorithm</h2></li>
<li>The Two Phase Commit Algorithm ensures that a sequence of events either runs to <code>successful completion</code>, or, if the sequence fails, that the overall system is <code>returned to its original state</code> as though nothing had happened. In other words, it allows intermediate steps that have occurred to be <code>undone</code> (i.e., rolledback) to return things back to a safe, sensible state, if a fault is detected.</li>
<li>A <code>coordinator node</code> requests a transaction, and sends a request to all participants nodes<ul>
<li>• e.g., to node C1, it sends ‘request to remove X pounds from account’, and to C2 sends ‘request to add X pounds to account’.</li>
</ul>
</li>
<li>All participants respond as to whether they are willing and able to execute the request, and send VOTE_COMMIT or VOTE_ABORT.<ul>
<li>They log their current state, and then perform the transaction</li>
<li>All participants log their vote</li>
</ul>
</li>
<li>The coordinator looks at the votes. If everyone has voted to commit, then the co-ordinator sends a GLOBAL_COMMIT to everyone; otherwise it sends a GLOBAL_ABORT.</li>
<li>On receiving the decision from the coordinator, all participants record the decision locally. If it was an ABORT, participants ROLL BACK to their previous safe state.<h2 id="Electing-a-Coordinator-Node"><a href="#Electing-a-Coordinator-Node" class="headerlink" title="Electing a Coordinator Node"></a>Electing a Coordinator Node</h2></li>
<li>The Bully Algorithm is a mechanism for choosing a coordinator from a set of candidate nodes.<ul>
<li>The algorithm gets its name from the fact that higher numbered nodes ‘bully’ lower numbered nodes into submission.</li>
</ul>
</li>
<li>Note 1: the algorithm relies on the use of timeouts to decide when to ‘give up’ waiting for responses from nodes that have potentially died (so the usual problem of ‘how long is it reasonable to wait’ applies here).</li>
<li>Note 2: the algorithm assumes that the participating nodes are ordered.</li>
<li>P sends an ELECTION message to all nodes with higher numbers.</li>
<li>If no one responds, P wins the election and becomes the co-ordinator. It informs all the other nodes that it is now the coordinator.</li>
<li>If one of the higher-numbered nodes Q answers, P concedes that it is not the winner, and Q begins the election process again until one node eventually wins.<h2 id="Clock-Synchronization"><a href="#Clock-Synchronization" class="headerlink" title="Clock Synchronization"></a>Clock Synchronization</h2></li>
<li>In distributed systems, clock synchronisation is not completely possible by the fact that messages are <code>not sent instantaneously</code> over real networks, and that there usually is some degree of <code>variation in the time</code> messages take <code>to arrive at their destination.</code></li>
<li>As long some amount of error is acceptable, there are at least two widely acceptable ways for getting clocks in different parts of a system into near-synchronisation.<ul>
<li>Cristian’s Algorithm</li>
<li>The Berkeley Algorithm<h3 id="Cristian’s-Algorithm"><a href="#Cristian’s-Algorithm" class="headerlink" title="Cristian’s Algorithm"></a>Cristian’s Algorithm</h3></li>
</ul>
</li>
<li>Cristian’s algorithm works between a process P, and a time server S connected to a source of UTC (Coordinated Universal Time).</li>
<li>It relies on the accuracy of an estimate based on the Round Trip Time (RTT), which is the elapsed timebetweenthetimewhenarequestissent from P to S, and the time when a response is received by P from S.</li>
<li>P requests the time from S.</li>
<li>After receiving the request from P,S prepares a response and appends the time T from its own clock. The response is sent to P.</li>
<li>P then sets its time to be T+RTT/2, where RTT is Round Trip Time of the request P made to S.<ul>
<li>This method assumes that the RTT is split equally between both request and response, which may not always be the case but is a reasonable assumption on a LAN connection.</li>
</ul>
</li>
<li>But what happens if the RTT estimate is not accurate, i.e., it is different when the update is sent to that which was measured?<ul>
<li>Clocks tend to drift (incorrect)</li>
</ul>
</li>
<li> And what happens if send and receive do not take the same amount of time, which affects RTT/2?</li>
<li> These issues are dealt with by a similar but more complex algorithm, The Berkeley Algorithm.<h3 id="The-Berkeley-Algorithm"><a href="#The-Berkeley-Algorithm" class="headerlink" title="The Berkeley Algorithm"></a>The Berkeley Algorithm</h3></li>
</ul>
<ol>
<li>A master is chosen via an election process such as the Bully Algorithm.</li>
<li>The master polls the slaves who reply with their time in a similar way to Cristian’s algorithm.</li>
<li>The master observes the round-trip time (RTT) of the messages and estimates the time of each slave and its own.</li>
<li>The master then averages the clock times, ignoring any values it receives far outside the values of the others.</li>
<li>Instead of sending the updated current time back to the other process, the master then sends out the amount (positive or negative) that each slave must adjust its clock. This avoids further uncertainty due to RTT at the slave processes.</li>
</ol>
<ul>
<li>Unlike Cristian’s algorithm, the server process in the Berkeley algorithm, called the master, periodically polls other slave processes.</li>
<li>The clock synchronisation method used in this algorithm, the average, cancels out individual clock’s tendencies to drift.<h1 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h1></li>
<li>Deadlock occurs when four particular conditions hold simultaneously in a system:</li>
</ul>
<ol>
<li>Mutual exclusion: at least one resource must be non-shareable, i.e. only one process can use that resource at a given time.</li>
<li>Hold and wait: a process is currently holding at least one resource, and is requesting at least one more resource that is currently being held by another process (i.e., it is waiting for a process to release something).</li>
<li>No pre-emption: once a process has acquired a resource, nothing (e.g., the operating system or some external factor) can force it to relinquish that resource; it has to do so voluntarily.</li>
<li>Circular wait: a process must be waiting for a resource that is being held by another process, which in turn and possibly indirectly, is waiting for the first process to release a resource.<ul>
<li>Put another way ,processes are waiting for one another in such a way that there is a cycle of dependencies between them. For example, if A waits for B, B waits for C, and C waits for A, we have a cycle of waiting that would fulfil this condition.<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2></li>
</ul>
</li>
</ol>
<ul>
<li>Consider a situation where two processes are running at the same time; one of them is using a printer, and the other, a scanner. It comes a point in time when:<ul>
<li>Both processes can no longer continue to do their work until they can get access to the device that is being used by the other process.</li>
<li>The processes will not relinquish control of the device they are currently using until they get it.</li>
</ul>
</li>
<li>The result is: now both processes are in a deadlock state—neither can continue because they are waiting for the other.<h2 id="Dealing-with-a-Deadlock"><a href="#Dealing-with-a-Deadlock" class="headerlink" title="Dealing with a Deadlock"></a>Dealing with a Deadlock</h2></li>
<li>There are three potentially sensible strategies for dealing with a deadlock:<ul>
<li><code>Prevent</code> deadlock from occurring in the first place by making sure that it is never possible for all four of the conditions to be true at the same time (you might like to think about what this means in each of the four cases).</li>
<li><code>Avoid</code> deadlock by making sure that, even though it is potentially possible for all four conditions to hold at the same time, that they never do.</li>
<li><code>Detect</code> a deadlock and deal with the consequences (this last approach is very similar to ‘avoid’ really, in that it requires having some mechanism for detecting whether all four conditions are about to hold (for avoid) or actually do hold (for detect)).</li>
<li>Whether you decide to go for a detect or avoid approach really depends on how likely or frequently a deadlock is likely to happen—if some property of your system means that deadlock is possible, but hugely unlikely, it might make sense to allow it to occur once in a while and to then deal with the consequences, rather than to incur the overhead of trying to avoid it from happening in the first place.</li>
</ul>
</li>
</ul>
<h1 id="Introduction-to-Basic-Concepts-Naming-Schemes"><a href="#Introduction-to-Basic-Concepts-Naming-Schemes" class="headerlink" title="Introduction to Basic Concepts: Naming Schemes"></a>Introduction to Basic Concepts: Naming Schemes</h1><ul>
<li>In distributed systems, names are used to <code>uniquely identify</code> entities, to refer to locations, and more.</li>
<li>An important issue with naming is that a name can be resolved to the entity it refers to; <code>name resolution</code> thus refers to the means by which a process is allowed to access a named entity, which is supported by a <code>naming system</code>.</li>
<li>The implementation of a naming system is itself often distributed across multiple machines.<h2 id="Example-Internet-Naming"><a href="#Example-Internet-Naming" class="headerlink" title="Example: Internet Naming"></a>Example: Internet Naming</h2></li>
<li>As in any distributed system, every computer connected to the <code>Internet</code> needs to be “addressable”, so that other computers on the net are able to “talk” to each other. Naming entities is the addressing mechanism via which a computer on the Internet is uniquely identified.<ul>
<li>The Internet is the biggest distributed system of all, being a huge network of networks.</li>
</ul>
</li>
<li>Possible approaches to addressing mechanisms include:<ul>
<li>Centralised</li>
<li>Free-for-all</li>
<li>By delegating (give) naming responsibilities<h2 id="Centralised-Naming-Approach"><a href="#Centralised-Naming-Approach" class="headerlink" title="Centralised Naming Approach"></a>Centralised Naming Approach</h2></li>
</ul>
</li>
<li>Centralised naming is the most obvious approach to guaranteeing that<code> any name is handed out once and only once</code>. In this approach, there is a <code>single point of contact</code>, that either validates that a name is unique, or alternatively makes up a unique name and hands that out on demand.</li>
<li>Its main limitation is that the <code>single point of contact</code> has to deal with every request, and, as a consequence, it is <code>not a very scalable solution</code>, and it creates <code>a single point of failure</code>.<h2 id="Free-for-All-Naming-Approach"><a href="#Free-for-All-Naming-Approach" class="headerlink" title="Free-for-All Naming Approach"></a>Free-for-All Naming Approach</h2></li>
<li>Free-for-all allows any object that wants a name to make up its own name.</li>
<li>Although this is a <code>massively ‘distributed’ solution</code> which avoids a single point of failure, it <code>does not guarantee uniqueness</code>.<h2 id="The-‘Delegating-Naming-Responsibilities’-Approach"><a href="#The-‘Delegating-Naming-Responsibilities’-Approach" class="headerlink" title="The ‘Delegating Naming Responsibilities’ Approach"></a>The ‘Delegating Naming Responsibilities’ Approach</h2></li>
<li>In this approach, the authority to allocate names is <code>delegated to smaller parts of the system</code>, and governed by some rules.</li>
<li>This approach <code>better balances</code> the conflicting issues associated with single points of failure and scalability, but it raises questions as to what rules are appropriate for each system.<ul>
<li>E.g., a rule could state that every device in a particular organisation has to have a name that includes the name of the organisation; another rule could state that every device in a particular country has to have a name that includes the name of the country; etc.<h2 id="MAC-Addresses"><a href="#MAC-Addresses" class="headerlink" title="MAC Addresses"></a>MAC Addresses</h2></li>
</ul>
</li>
<li>The Media Access Control (MAC) address is a unique identifier given to each network device in a system: this means that every ethernet or wifi card in a computer has one MAC address.</li>
<li>Note that, since most computers have several network devices, there are more MAC addresses than there are computers. A MAC address is a 48 bit number, so that means there are 281,474,976,710,656 different numbers, or enough for every person living on Earth to have lots of different network devices.</li>
<li>A MAC consists of two main parts: the ‘Organisationally Unique Identifier’ (OUI) and the ’Network Interface Controller’ (NIC).</li>
<li>A MAC address does not tell you where a device is on a network.</li>
<li>The OUI is a 24 bit number that is purchased from the Institute of Electrical and Electronics Engineers (IEEE), which acts as a central authority from which vendors (selling provider) of hardware can purchase unique identifiers. Once a vendor has an OUI to use in the MAC addresses for its hardware, as long as that particular vendor makes sure that the NIC part of the address is unique, then the combination of the OUI and the NIC will be unique.<h2 id="IP-Addresses"><a href="#IP-Addresses" class="headerlink" title="IP Addresses"></a>IP Addresses</h2></li>
<li>An Internet Protocol Address (IP Address) serves two purposes: it is a ‘unique’ identifier and also contains some information about ‘where’ a device is on a network.</li>
<li>Most IP addresses are 32 bit numbers, but are most often written as four 8 bit numbers separated by dots (e.g. 130.88.192.9).</li>
<li>The top-level authority for IP addresses is the Internet Assigned Numbers Authority (IANA), which delegates ranges of the address space to five Regional Internet Registries, which in turn delegate sub ranges of their space to Internet service providers.</li>
<li>Note that, unlike MAC addresses, the delegation of IP addresses takes place initially to <code>geographical regions</code> (rather than ‘hardware vendors’),</li>
<li>And, for this reason, <code>IP addresses can tell you some information about the location of a device on a network</code>.</li>
<li>There are not enough IP addresses for each person on Earth, and so, <code>IPv6W</code>, the latest generation of IP address, uses 128 bit numbers to overcome this problem.<h2 id="Domain-Names"><a href="#Domain-Names" class="headerlink" title="Domain Names"></a>Domain Names</h2></li>
<li>Domain Names were created because humans find IP addresses hard to read.</li>
<li>The Domain Name System (DNS) is itself a Distributed System built on top of the Internet, used to <code>create associations between human-readable names and IP addresses.</code><ul>
<li>For example, <a target="_blank" rel="noopener" href="http://www.bbc.co.uk/">www.bbc.co.uk</a>, will be “translated into” an IP address that you can use to communicate with the appropriate computer.</li>
</ul>
</li>
<li>The ‘delegation’ model used by the DNS is complex, as it has aspects of geographical delegation<ul>
<li>For example, domain names ending in ‘.co.uk’ are for UK-based companies.</li>
</ul>
</li>
<li>But it also separates matters out in other ways as well.<ul>
<li>For example, domains ending in ’.com’ and ’.org’ have no geographical implications, but refer to companies and organisations.</li>
</ul>
</li>
<li>Unlike MAC and IP addresses, DNS cannot allocate ‘batches’ of names up- front, and needs to respond in real-time to requests to translate names into IP addresses.<br>• It achieves this by being in itself a Distributed System, <code>consisting of a hierarchy of servers</code> with the most authoritative server at the ‘top’ of the hierarchy, dealing with requests from users.</li>
</ul>
<h1 id="Introduction-to-Basic-Concepts-Protocols"><a href="#Introduction-to-Basic-Concepts-Protocols" class="headerlink" title="Introduction to Basic Concepts: Protocols"></a>Introduction to Basic Concepts: Protocols</h1><ul>
<li>Protocols define sets of rules governing how two or more objects should interact with one another. Protocols serve as specifications rather than implementations of a piece of technology.</li>
<li>An example of an relevant protocol is the <code>HyperText Transport Protocol </code>(HTTP), which provides an specification (i.e., a vocabulary) that allows client applications to <code>request resources</code> from Web servers, and Web servers to <code>respond</code> to these requests.</li>
<li>Examples of “verbs” used by client applications and Web servers when communicating via the HTTP protocol over the Internet are shown below.<ul>
<li>HEAD - Asks for the response identical to the one that would correspond to a GET request, but without the response body. This is useful for retrieving meta-information written in response headers, without having to transport the entire content.</li>
<li>GET - Requests a representation of the specified resource. Requests using GET should only retrieve data and should have no other effect.</li>
<li>POST - ubmits data to be processed (e.g., from an HTML form) to the identified resource. The data is included in the body of the request. This may result in the creation of a new resource or the updates of existing resources or both.</li>
<li>OPTIONS - Returns a list of the commands supported by this particular server.</li>
<li>DELETE - It is used to delete a resource. It may return the a representation of the removed resource.<h2 id="Statelessness"><a href="#Statelessness" class="headerlink" title="Statelessness"></a>Statelessness</h2></li>
</ul>
</li>
<li>By following the HTTP protocol, Web servers are said to be stateless, meaning that once a request from a client application is fulfilled, the Web <code>server disconnects from the client and “forgets” that the client ever connected.</code></li>
<li>The stateless nature of the communication between client and server allows the system to treat each request for content as <code>an independent transaction </code>that can be completed.</li>
<li>However, there are ways in which state information can be preserve. These are outside of the scope of the protocol, but encoded in applications and servers that use it.<h2 id="Email"><a href="#Email" class="headerlink" title="Email"></a>Email</h2></li>
<li>Electronic mail (email) seems like a simpler system than ‘the Web’, in many ways it is more complex.</li>
<li>The expectation is that an email has to be in exactly one place at any one time.</li>
<li>If it is in two places at the same time then it has been duplicated accidentally.</li>
<li>If it is in zero places then it has been lost ‘in the system’, in detriment(damage) of the sender and the recipient of that email.<h2 id="Email-Associated-Protocols"><a href="#Email-Associated-Protocols" class="headerlink" title="Email Associated Protocols"></a>Email Associated Protocols</h2>The Simple Mail Transport Protocol (SMTP)</li>
<li>Like HTTP, SMTP is a text-based protocol. But unlike HTTP, it is ‘connection based’, meaning that a client (in this case, the Mail User Agent) can <code>issue multiple consecutive (following) comments</code> to the SMTP server, and should explicitly terminate its connection when its finished.</li>
<li>At the end of this exchange, Bob’s email has successfully been moved from his mail client, to his ‘outgoing mail server’.</li>
</ul>
<h1 id="Sequential-vs-Multi-Processing-Concurrent-Parallel-and-Distributed-Computing"><a href="#Sequential-vs-Multi-Processing-Concurrent-Parallel-and-Distributed-Computing" class="headerlink" title="Sequential vs. Multi Processing, Concurrent, Parallel and Distributed Computing"></a>Sequential vs. Multi Processing, Concurrent, Parallel and Distributed Computing</h1><ul>
<li>A machine makes available computing and storage resources.</li>
<li>By computing resource we mean, e.g., a processor, such as a CPU.</li>
<li>By storage resource we mean, e.g., a given amount of primary memory.</li>
<li>A process is an executing instance of a program.</li>
<li><code>Resource usage</code> is typically <code>controlled</code> by an operating system (<code>OS</code>).</li>
<li>Since resources are scarce (rare) and differ in their capabilities, an OS aims to make <code>the most efficient use possible</code> of those resources.</li>
<li>The OS assigns a <code>unique identity to each process</code> and then controls how a process is granted access to computing resources.</li>
<li>The OS also controls how a process is granted access to storage resources by assigning an address space to that process.</li>
<li>When the OS ensures that<code> each process P has a single address space A that is exclusive (only) to P</code> , we are allowed a <code>sequential reading of the steps that comprise the process</code>.<h2 id="Sequential-Processing"><a href="#Sequential-Processing" class="headerlink" title="Sequential Processing"></a>Sequential Processing</h2></li>
<li>If foo and bar take long to run, we may wish to run them <code>concurrently, in different processes,</code> and perhaps even better, in <code>parallel, in different processors</code>.</li>
<li>If foo and bar are proprietary services held in remote machines, we may not be able to hold local copies of them.</li>
<li>Sequential, isolated processing is simple , but <code>bounded and limiting</code>.</li>
<li><code>Non sequential, non isolated</code> processing expands the bounds and limits with respect to <code>performance</code>.</li>
<li>It is possible to switch between a process P and a process P’ if, for example, P is idle (free) waiting for something (like I/O) to complete; also, we get more responsiveness, as:<ul>
<li>While printing a long document, your machine still allows you to go on doing other things.</li>
<li>While downloading a file, a web browser still allows you to traverse (iterate) a link.</li>
<li>While you ponder(think) what to do next, the same machine goes about attending to someone else.<h2 id="Multi-Processing-vs-Multi-Tasking"><a href="#Multi-Processing-vs-Multi-Tasking" class="headerlink" title="Multi-Processing vs. Multi-Tasking"></a>Multi-Processing vs. Multi-Tasking</h2></li>
</ul>
</li>
<li>Two different concepts:</li>
<li>An OS multi tasks by:<ul>
<li>allowing more than one process to be underway by <code>controlling</code> how each one makes use of the <code>resources</code> allocated to it</li>
<li>implementing a scheduling policy, which grants <code>each active process</code> a time slice during which it can <code>access the resources</code> allocated to it.</li>
</ul>
</li>
<li>Taking it literally, in multi-tasking, processes are <code>not really executing concurrently.</code><ul>
<li>Concurrent execution is <code>only apparent</code>.</li>
</ul>
</li>
<li>The appearance of concurrent execution stems from an effective scheduling policy.</li>
<li>If all processes get a fair share of the resource and they get it sufficiently often, it <code>seems</code> to users that all processes are executing concurrently.</li>
<li>For example, while a process P is waiting on a slow output device, the OS may schedule another process P’ to make use of the CPU. It seems to users that the machine is <code>both printing for P and running P’.</code><h2 id="Multi-Processing-by-Forking"><a href="#Multi-Processing-by-Forking" class="headerlink" title="Multi-Processing by Forking"></a>Multi-Processing by Forking</h2></li>
<li>When a process forks (using an OS call) it <code>causes two copies of itself</code> to be active concurrently.</li>
<li>The child process is given <code>a copy of</code> the <code>parent process’s address space.</code> The <code>address spaces however are distinct</code>. And so, if either process modifies a variable in its address space, this change is not visible to the other process.<ul>
<li>The child process starts executing after the OS call.</li>
<li>The parent can continue or wait for the child to execute.</li>
<li>Ultimately (finally) the parent must mean to <code>find out how and when the child completes execution.</code></li>
</ul>
</li>
<li>Forking is quite common in the client-server type of distributed computing, as a server typically forks a child process for each request it receives</li>
<li>Because of the <code>copying</code> , forking can be <code>expensive.</code></li>
<li>In practice, <code>modern OSs</code> have strategies that make the actual cost quite <code>affordable.</code></li>
<li>Forking is reasonably safe because the address spaces are distinct<ul>
<li>Discipline in adhering to best practice is nonetheless <code>required </code>(e.g., to avoid zombie processes, to avoid unintended sharing of references to files, etc.)<h2 id="Multi-Processing-by-Threading"><a href="#Multi-Processing-by-Threading" class="headerlink" title="Multi-Processing by Threading"></a>Multi-Processing by Threading</h2></li>
</ul>
</li>
<li>Forking imposes (increase) a certain degree of isolation. And so, if parent and child need to <code>interact and share</code>, threading may be a better approach to multi tasking.</li>
<li>With threading, <code>the address space is not copied, it is shared.</code><ul>
<li>This means that if one process changes a variable, all other processes see it.</li>
<li>This makes threading <code>less expensive</code>, but also <code>less safe</code> than forking.<h2 id="Concurrent-Computing"><a href="#Concurrent-Computing" class="headerlink" title="Concurrent Computing"></a>Concurrent Computing</h2></li>
</ul>
</li>
<li>Consider many application processes.</li>
<li>Processes are often threads</li>
<li>The OS schedules the execution of n copies of a process Pi, 1 =&lt; i=&lt; n, to run in the same processor, typically sharing a single address space.<h2 id="Parallel-Computing"><a href="#Parallel-Computing" class="headerlink" title="Parallel Computing"></a>Parallel Computing</h2></li>
<li>There are now many processors bound by an interconnect (e.g., a bus across processors).</li>
<li>There is truly many processes running at the same time, not just multi threading, but true parallelism.</li>
<li>The n copies of a process Pi, 1 =&lt;i =&lt; n, can, each, run in one of m, 1<br>=&lt;j =&lt; m, different processors Cj, possibly (but not necessarily)<br>sharing a single address space A.<h2 id="Distributed-Computing"><a href="#Distributed-Computing" class="headerlink" title="Distributed Computing"></a>Distributed Computing</h2></li>
<li>There are many independent, self-sufficient, autonomous, heterogeneous machines.</li>
<li>We now have spatial (in space) separation.</li>
<li><code>Message exchange</code> is needed, <code>network effects</code> are felt.</li>
<li>Complexity may reach a point in which applications are not written against OS services. Instead, they are written against a <code>middleware API</code>. The middleware then takes some of the complexity upon itself.</li>
<li>The n (not necessarily identical) processes Pi, 1 =&lt; i =&lt; n, each run in one of m, 1 =&lt; j =&lt; m, <code>different machines Mj</code>, that <code>cannot</code> share a single address space A (and therefore must communicate).</li>
</ul>
<h1 id="Architectures-of-Distributed-Systems"><a href="#Architectures-of-Distributed-Systems" class="headerlink" title="Architectures of Distributed Systems"></a>Architectures of Distributed Systems</h1><ul>
<li>An obvious way to distinguish between distributed systems is on the organisation of their <code>software components </code>and how they interact. In other words, their <code>software architecture</code>.</li>
<li>Centralised architectures, e.g., traditional client-server, where a <code>single server</code> implements most of the software components (and thus functionality), while <code>remote clients can access</code> that server using simple communication means.</li>
<li>Decentralised architectures, e.g., <code>peer-to-peer architectures</code> in which all nodes more or less play equal roles.</li>
<li>Hybrid architectures, i.e., combining elements from centralized and decentralized architectures.<h2 id="Software-Architectural-Styles"><a href="#Software-Architectural-Styles" class="headerlink" title="Software Architectural Styles"></a>Software Architectural Styles</h2></li>
<li>A software architectural style is formulated in terms of <code>components</code>, the way that <code>components are connected to each other</code>, the <code>data exchanged</code> between components, and finally how these elements are jointly configured into a system.<ul>
<li>A component is a <code>modular unit</code> with well-defined required and <code>provided interfaces</code> that is replaceable within its environment.</li>
<li>A connector is a mechanism that  <code>mediates communication, coordination, or cooperation among components </code>. In other words, a connector allows for the  <code>flow of control and data between components </code>.</li>
</ul>
</li>
<li>A variety of architectural styles result from the creation of systems configurations using components and connectors, including:<ul>
<li>Layered architectural styles<ul>
<li> In this architectural style, components are organized in a layered fashion where a component at layer Lj can make a downcall to a component at a lower-level layer Li (with i &lt; j) and generally expects a response.</li>
<li> The layers on the bottom <code>provide a service to the layers on the top</code>. The <code>request flows from top to bottom</code>, whereas the <code>response is sent from bottom to top</code>. In this approach, the calls always follow a predefined path.</li>
</ul>
</li>
<li>Object-based architectural styles<ul>
<li>In this architectural style, each object <code>corresponds to a component</code>, and these components are connected through a <code>procedure call mechanism</code>, that can take place over a network, if the calling object is not on the same machine as the called object.</li>
<li>Object-based architectures provide a <code>natural way of encapsulating data </code>and the operations that can be performed on that data into a single entity.</li>
<li>And so, communication between objects happen as method invocations. These are generally called Remote Procedure Calls (RPC).</li>
</ul>
</li>
<li>Resource-centered architectural styles<ul>
<li>In this architectural style, a distributed system is viewed as <code>a huge collection of resources</code> that are individually managed by components. Resources may be added or removed by (remote) applications, and likewise can be retrieved or modified.</li>
<li>It is based on a <code>data center</code>, where the primary communication happens via a central data repository.</li>
<li>This approach has been widely adopted for the Web.</li>
</ul>
</li>
<li>Event-based architectural styles<ul>
<li>In this architectural style, processes running on the various components are <code>both referentially decoupled</code> and <code>temporally coupled</code>. In other words, one process does not explicitly know any other process, but for coordination to take place processes need to be <code>running at the same time</code>.</li>
<li>In such scenarios, the only thing a process can do is <code>publish a notification</code> describing the occurrence of an event).</li>
<li>Assuming that notifications come in all sorts and kinds, processes may subscribe to a specific kind of notification.</li>
<li>Component send event notification to <code>Event bus</code>, and the bus deliver to other component<h2 id="Centralised-System-Architectures-Examples"><a href="#Centralised-System-Architectures-Examples" class="headerlink" title="Centralised System Architectures: Examples"></a>Centralised System Architectures: Examples</h2>simple Client-Server Architectures</li>
</ul>
</li>
</ul>
</li>
<li>Processes in a distributed system are divided into two main groups: Clients and Servers.</li>
<li>A <code>server</code> is a process implementing a specific service, for example, a file system service or a database service.</li>
<li>A<code> client</code> is a process that <code>requests</code> a service from a server by sending it a request and, subsequently, waiting for the server’s reply.</li>
<li>The client-server interaction is known as request-reply behaviour.<h2 id="Multi-Tiered-Client-Server-Architectures"><a href="#Multi-Tiered-Client-Server-Architectures" class="headerlink" title="Multi-Tiered Client-Server Architectures"></a>Multi-Tiered Client-Server Architectures</h2></li>
<li>Typically presents three logical tiers (layer).</li>
<li>The distinction (difference) into <code>three logical tiers</code> suggests a number of possibilities for physically distributing a client-server application across several machines. However, the simplest organization is to have only two types of machines:<ul>
<li>A <code>client</code> machine containing only (part of) the <code>user-interface level</code>.</li>
<li>A <code>server</code> machine containing the rest, i.e., the programs implementing the processing and data management functionalities.</li>
</ul>
</li>
<li>In the most typical organization, all functionality is handled by the server, while the client is essentially no more than a dumb terminal.</li>
<li>Many distributed applications are divided into the three layers.<ul>
<li>User interface layer, </li>
<li>Processing layer,</li>
<li>Data layer.</li>
</ul>
</li>
<li>Thus, the main challenge to clients and servers is to distribute these layers across different machines.</li>
<li>A server may sometimes need to act as a client, as shown, typically leading to a physically three-tiered architecture.</li>
<li>An example of use of this architecture is in the organisation of Web sites.</li>
</ul>
<p><img src="https://www.researchgate.net/profile/Trevor-Mudge/publication/221147997/figure/fig1/AS:339672459956226@1457995633431/A-Typical-3-Tier-Server-Architecture-Tier-1-Web-Server-Tier-2-Application-Server-Tier.png"></p>
<ul>
<li>Multi-tiered client-server architectures are a consequence of dividing distributed applications into a <code>user interface</code>, <code>processing</code>, and <code>data-management components</code>, where the different tiers correspond directly with the logical organization of applications. This type of distribution is called <code>vertical distribution</code>, achieved by placing logically different components on different machines.<h2 id="Decentralised-System-Architectures-Peer-to-Peer-Systems"><a href="#Decentralised-System-Architectures-Peer-to-Peer-Systems" class="headerlink" title="Decentralised System Architectures: Peer-to-Peer Systems"></a>Decentralised System Architectures: Peer-to-Peer Systems</h2></li>
<li>In decentralised architectures, there is a greater concern about distributing client and server functionality more evenly across machines to achieve better <code>workload balance</code>.</li>
<li>As a consequence, a client (or a server) may be physically split up into a number of logical parts, with each part operating on “its own share of the complete data set”. This is a <code>horizontal distribution </code>of functionality.</li>
<li>A class of modern system architectures that support this horizontal distribution is known as peer-to-peer systems.</li>
<li>From a high-level perspective, the processes that constitute a peer-to-peer system are all equal.</li>
<li>Much of the interaction between processes is symmetric: <code>each process will act as a client and a server</code>.</li>
<li>Due to the symmetric behaviour of processes in peer-to-peer architectures, processes are organised in an <code>overlay network</code>;<ul>
<li>i.e., its nodes are formed by the processes and its links represent the possible communication channels (<code>TCP connections</code>).<ul>
<li>The Transmission Control Protocol (TCP) is one of the main Internet Protocols.</li>
</ul>
</li>
<li>Thus node may not be able to communicate directly with an arbitrary other node, but is required to send messages through the available communication channels.</li>
</ul>
</li>
<li>Two types of overlay networks exist, characterising peer-to-peer systems as:<ul>
<li>Structured</li>
<li>Unstructured<h2 id="Structured-Peer-to-Peer-Systems"><a href="#Structured-Peer-to-Peer-Systems" class="headerlink" title="Structured Peer-to-Peer Systems"></a>Structured Peer-to-Peer Systems</h2></li>
</ul>
</li>
<li>In structured P-2-P systems, <code>nodes are organized</code> in an overlay that adheres (persist) to a <code>specific</code>, deterministic <code>topology</code>: a ring, a binary tree, a grid, etc.</li>
<li>This topology is used to <code>efficiently look up data</code> that is maintained by the system,<ul>
<li>i.e., each data item is <code>uniquely</code> associated with a <code>key</code>, typically obtained by a hash function on the data item’s value. This key is used as an index, since it identifies a node in the system.</li>
<li>key(data item) = hash(data item’s value)</li>
</ul>
</li>
<li>The topology of a structured peer-to-peer system plays a crucial role: <code>any node</code> can be asked to look up a given key, i.e., to <code>efficiently route a request for data to the node responsible for storing the data associated with the given key.</code><h2 id="Structured-P2P-System-example"><a href="#Structured-P2P-System-example" class="headerlink" title="Structured P2P System example"></a>Structured P2P System example</h2></li>
<li>A peer-to-peer system with a fixed number of nodes, organised into a hypercube.</li>
<li>Each data item is associated with one of the 16 nodes of the hypercube - by hashing the value of a data item to a key k ε {0, 1, 2, . . .,24 –1}.<h2 id="Unsrtuctured-Peer-to-Peer-Systems"><a href="#Unsrtuctured-Peer-to-Peer-Systems" class="headerlink" title="Unsrtuctured Peer-to-Peer Systems"></a>Unsrtuctured Peer-to-Peer Systems</h2></li>
<li>In an unstructured peer-to-peer system <code>each node</code> maintains an <code>ad hoc list of neighbours</code>, such that the resulting overlay resembles a <code>random graph</code>: a graph in which an edge &lt;u, v&gt; between two nodes u and v exists only with a certain probability P[&lt;u, v&gt;].</li>
<li>When a node joins in, it often contacts a well-known node to obtain a starting list of other peers in the system. This list can then be used to find more peers, and perhaps ignore others, and so on. In practice, a node generally <code>changes its local list almost continuously</code>.</li>
<li>Unlike structured P-2-P systems, looking up data cannot follow a predetermined route, because lists of neighbours are constructed in an ad hoc fashion. Instead, <code>searching for data is necessary</code>.<h2 id="Example-of-Searching-Methods"><a href="#Example-of-Searching-Methods" class="headerlink" title="Example of Searching Methods"></a>Example of Searching Methods</h2><h3 id="Flooding"><a href="#Flooding" class="headerlink" title="Flooding"></a>Flooding</h3></li>
<li>Assume an issuing node, u, passes a request for a data item to all its neighbours.</li>
<li>Each of its neighbours, v:<ul>
<li>Ignores the request when it has seen it before, otherwise, searches locally for the requested data item.</li>
<li>If it has the required data, it can either respond directly to the issuing node u, or send it back to the original forwarder, who will then <code>return it to its original forwarder</code>, and so on.</li>
<li>If it does not have the requested data, it <code>forwards the request</code> to <code>all of its own neighbours</code>.</li>
</ul>
</li>
<li>Obviously, flooding is <code>expensive</code>, for which reason a request often has an associated time-to-live or TTL value, giving the maximum number of hops a request is allowed to be forwarded.<h3 id="Random-Walks"><a href="#Random-Walks" class="headerlink" title="Random Walks"></a>Random Walks</h3></li>
<li>An issuing node, u, tries to find a data item by asking a randomly chosen neighbour, v.</li>
<li>If v does not have the data, it forwards the request to one of its randomly chosen neighbours, and so on.</li>
<li>Generally, a random walk imposes less network traffic than Flooding, but it may take longer before a node is reached that has the requested data. To decrease the waiting time, an issuer can <code>simply start n random walks simultaneously</code>.</li>
<li>A random walk also needs to be stopped. To this end, a TTL can be used, or alternatively, when a node receives a lookup request, it can check with the issuer whether forwarding the request to another randomly selected neighbour is still needed.</li>
<li>Notably in unstructured P-2-P systems, <code>locating relevant data items</code> can become problematic as the network grows, causing a scalability problem.<h2 id="Making-Data-Search-more-Scalable-in-Unstructured-P-2-P-Systems"><a href="#Making-Data-Search-more-Scalable-in-Unstructured-P-2-P-Systems" class="headerlink" title="Making Data Search more Scalable in Unstructured P-2-P Systems"></a>Making Data Search more Scalable in Unstructured P-2-P Systems</h2></li>
<li>To improve <code>scalability</code> of data search, P-2-P systems can make use of <code>special nodes</code> that maintain an index of data items, abandoning their symmetric nature by creating “special” collaborations among <code>nodes</code>.</li>
<li>For example, in a collaborative content delivery network (CDN), nodes may offer storage for hosting copies of Web documents, allowing Web clients to access pages nearby, and thus to <code>access them quickly</code>.<h2 id="Hybrid-Architectures"><a href="#Hybrid-Architectures" class="headerlink" title="Hybrid Architectures"></a>Hybrid Architectures</h2></li>
<li>Encompass classes of distributed systems in which client-server solutions are combined with decentralized architectures. Examples:<ul>
<li>Edge-server systems</li>
<li>Collaborative distributed systems</li>
</ul>
</li>
<li>One of the main motivations for these hybrid architectures, is the <code>scalability problems</code> of unstructured peer-to-peer systems, and the difficulties in <code>workload balancing</code> in traditional client-server architectures.<h2 id="Collaborative-Distributed-Systems"><a href="#Collaborative-Distributed-Systems" class="headerlink" title="Collaborative Distributed Systems"></a>Collaborative Distributed Systems</h2></li>
<li>Combine traditional client-server structures (when nodes are joining the system) and fully decentralised structures (once a node has joined the system). An example of such a system is BitTorrent, a peer-to-peer file downloading system.</li>
<li>In BitTorrent, an end user, looking for a file, downloads chunks of the file from other users, until the downloaded chunks can be assembled together, yielding the complete file.<ul>
<li>To download a file, a user needs to <code>access a global directory</code> containing references to torrent files.</li>
<li>A torrent file contains the <code>information</code> that is needed to download a specific file, such as a link to a file <code>tracker</code>, a server that keeps an accurate account of active nodes that have (chunks of) the requested file.<ul>
<li>There will be many different trackers, although there will generally be only a single tracker per file (or collection of files).</li>
</ul>
</li>
<li>Once the nodes have been identified from where chunks can be downloaded, the downloading node effectively becomes active.<h2 id="Edge-Server-Systems"><a href="#Edge-Server-Systems" class="headerlink" title="Edge-Server Systems"></a>Edge-Server Systems</h2></li>
</ul>
</li>
<li>Are characterised by the following main properties:<ul>
<li><code>Are deployed on the Internet</code></li>
<li>Their <code>servers are placed “at the edge” of the network</code> (i.e., the <code>boundary</code> between enterprise networks and the actual Internet).</li>
</ul>
</li>
<li>The edge server’s main purpose is to serve content, possibly after applying filtering and transcoding functions.<ul>
<li>For a specific organization, one edge server acts as an <code>origin server from which all content originates</code>. That server can use other edge servers for <code>replicating high-demand content</code>, e.g., Web pages.</li>
</ul>
</li>
<li>Edge-server systems have recently been used to <code>assist data centers in cloud computations and storage</code>, leading to distributed cloud systems. In the case of fog computing, even end-user devices form part of the system and are (partly) controlled by a cloud-service provider.</li>
</ul>
<h1 id="Inter-Process-Communication"><a href="#Inter-Process-Communication" class="headerlink" title="Inter-Process Communication"></a>Inter-Process Communication</h1><ul>
<li>Inter-process communication encompasses (include) the ways that <code>processes on different machines</code> can <code>exchange information</code>.</li>
<li>Traditional inter-process communication has always been based on <code>low-level message passing</code>, as offered by the underlying network; thus it is <code>harder</code> to realise than communication based on shared memory, as available for non-distributed platforms.</li>
<li>As modern distributed systems often consist of thousands or even millions of processes scattered (dispersed) across a network with unreliable communication such as the Internet, development of large-scale distributed applications is very difficult.</li>
<li>Two widely-used models for communication are: <ul>
<li>Remote Procedure Call (RPC)<ul>
<li>Communication transparency cannot be achieved with traditional inter-process communication, as low-level operations (e.g., send and receive) do not conceal (hide) communication.</li>
<li>Based on the idea that programs can call procedures located on other machines, an RPC aims at<code> hiding</code> most of the intricacies of <code>message passing</code>, and is ideal for <code>client-server applications.</code></li>
<li>For example: when a process on machine A calls a procedure on machine B, the calling process on A is suspended (stopped), and execution of the called procedure takes place on B. Information can then be transported from the caller to the callee in the parameters and can come back in the procedure result. This way, no message passing is visible to the programmer.</li>
<li>A more detailed example<ul>
<li>A program has access to a database that allows it to append data to a stored list, after which it returns a reference to the modified list. The operation is made available to a program by means of a routine append: <code>newlist = append(data, dbList)</code></li>
<li>When append is a remote procedure, a different version of append (a.k.a. <code>client stub</code>) is offered to the calling client. The client stub packs the parameters into a message and requests that message to be sent to the server, by calling send. Following the call to send, the client stub <code>calls receive</code>, <code>blocking itself</code> until the reply comes back.</li>
<li>When the message arrives at the server, the server’s OS passes it to a server stub. The server stub unpacks the parameters from the message and then calls the server procedure in the usual way. The server performs its work and then returns the result to the caller (the server stub).<ul>
<li>A <code>server stub</code> is the server-side equivalent of a client stub: it is a piece of code that transforms requests coming in over the network into local procedure calls.</li>
</ul>
</li>
</ul>
</li>
<li>In summary:</li>
</ul>
<ol>
<li>The client procedure calls the client stub in the normal way.</li>
<li>The client stub builds a message and calls the local operating system.</li>
<li>The client’s OS sends the message to the remote OS.</li>
<li>The remote OS gives the message to the server stub.</li>
<li>The server stub unpacks the parameter(s) and calls the server.</li>
<li>The server does the work and returns the result to the stub.</li>
<li>The server stub packs the result in a message and calls its local OS.</li>
<li>The server’s OS sends the message to the client’s OS.</li>
<li>The client’s OS gives the message to the client stub.</li>
<li>The stub unpacks the result and returns it to the client.</li>
</ol>
<ul>
<li>Parameter Passing in RPCs <ul>
<li>The function of the client stub is to take its parameters, pack them into a message, and send them to the server stub.</li>
<li><code>Packing parameters</code> into a message is called <code>Parameter Marshalling</code> (Not straightforward).</li>
<li>The server just sees a series of bytes coming, which constitute (compose) the original message sent by the client, i.e., no additional information on how those bytes should be interpreted is provided with the message.</li>
<li>The meta-information can be recognized as original message by transforming data into a <code>machine-and network-independent format</code>, making sure that <code>both communicating parties expect the same message data type</code> to be transmitted.<ul>
<li>The latter can typically be solved at the level of programming languages.</li>
<li>The former is accomplished (finish) by using machine-dependent routines that transform data to and from machine- and network-independent formats.</li>
</ul>
</li>
<li>Marshalling is all about this transformation to <code>neutral (fair) formats and forms</code>, an essential part of remote procedure calls.</li>
</ul>
</li>
<li>Passing References to Objects<ul>
<li>Pointers or references passed by <code>copying the entire data structure </code>to which the parameter is referring, effectively <code>replacing the copy-by-reference</code> mechanism by <code>copy-by-value/restore</code>.</li>
</ul>
</li>
<li>Asynchronous RPCs<ul>
<li> To support situations in which there is simply <code>no result to return to the client</code>, RPC systems may provide facilities for what are called asynchronous RPCs.</li>
<li> With asynchronous RPCs, the server <code>immediately sends a reply back</code> to the client the moment the RPC request is received, after which it locally calls the requested procedure. The reply acts as an <code>acknowledgment</code> to the client that the server is going to process the RPC. The client will <code>continue</code> without further blocking as soon as it has received the server’s acknowledgment.</li>
<li>The feedback is only means the remote procedure has received the client’s call!</li>
</ul>
</li>
</ul>
</li>
<li>Message-Oriented Middleware (MOM)<ul>
<li>MOM relies on <code>message-queuing</code> mechanisms for providing extensive support for <code>persistent asynchronous communication</code>. As such, message-queuing systems offer intermediate(middle)-term storage capacity for messages, without requiring either the sender or receiver to be active during message transmission.</li>
<li>Message-queuing systems are typically targeted to support message transfers that are <code>allowed to take minutes</code> instead of seconds or milliseconds.</li>
<li>The sender posts a message in the queue, and the receiver retrieves the message from the queue.</li>
<li>Applications communicate by inserting messages in specific queues which are then forwarded until eventually delivered to the destination.</li>
<li>A sender is generally given only the guarantees that its message will <code>eventually</code> be inserted in the recipient’s queue.</li>
<li>Message-queuing systems are often used to assist the <code>integration</code> of dispersed collections of databases as well as in <code>publish-subscribe </code>systems.</li>
<li>“Message queues enable <code>asynchronous</code> communication, which means that the endpoints that are producing and consuming messages interact with the queue, not each other. Producers can <code>add requests</code> to the queue <code>without waiting for them</code> to be processed. Consumers process messages only when they are available. No component in the system is ever stalled (stopped) waiting for another, optimizing data flow.”</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Replication-of-Data-in-Distributed-Systems"><a href="#Replication-of-Data-in-Distributed-Systems" class="headerlink" title="Replication of Data in Distributed Systems"></a>Replication of Data in Distributed Systems</h1><h2 id="Data-Replication"><a href="#Data-Replication" class="headerlink" title="Data Replication"></a>Data Replication</h2><ul>
<li>Replication is used to<ul>
<li>Enhance system <code>reliability</code></li>
<li>Improve performance. </li>
<li>Potentially improve system <code>scalability</code></li>
</ul>
</li>
<li>Major challenge: <code>keeping data replicas consistent</code><h2 id="Increasing-Reliability-through-Replication"><a href="#Increasing-Reliability-through-Replication" class="headerlink" title="Increasing Reliability through Replication"></a>Increasing Reliability through Replication</h2>Motivation:</li>
<li>If a file system has been replicated, it may be possible to <code>continue working with it after one replica crashes</code>, by simply switching to one of the other replicas.</li>
<li>By maintaining multiple replicas of the same data, it becomes possible to provide better <code>protection against corrupted 损坏的 data</code>.<h2 id="Improving-Performance-through-Replication"><a href="#Improving-Performance-through-Replication" class="headerlink" title="Improving Performance through Replication"></a>Improving Performance through Replication</h2>Motivation:</li>
<li>A distributed system needs to scale in terms of <code>size</code><ul>
<li>E.g., when an increasing number of processes needs to access data that are managed by a single server. 当越来越多的进程需要访问单独的服务器的数据时</li>
</ul>
</li>
<li>A distributed system needs to scale in terms of the <code>geographical area</code> it covers<ul>
<li>E.g., by placing a copy of data in proximity of the process using them，the time to access the data decreases. As a consequence, performance, as perceived by that process, increases.</li>
<li>通过将数据副本放置在使用它们的进程附近，访问数据的时间会减少。 因此，该过程所感知的性能会提高。</li>
<li>Note that, although a client process may perceive better performance, it may also be the case that <code>more network bandwidth</code> is consumed to <code>keep all replicas</code> up to date<h2 id="The-Price-of-Replication"><a href="#The-Price-of-Replication" class="headerlink" title="The Price of Replication"></a>The Price of Replication</h2><blockquote>
<p>The problem with replication is that having multiple copies may lead to <code>consistency problems</code>.</p>
</blockquote>
</li>
</ul>
</li>
<li>Whenever a copy is modified, that copy becomes <code>different</code> from the other copies. </li>
<li>Consequently, modifications have to be carried out on <code>all copies</code> to ensure consistency. </li>
<li>Exactly <code>when and how</code> those modifications need to be carried out determines the <code>price of replication</code>.<br>Example - Trying to improve access times to Web pages:</li>
<li>To imporve performance, Web browsers often <code>cache</code> a Web page<ul>
<li>Pros: excellent access time from a user viewpoint</li>
<li>Cons: requires refetching for latest version of a page </li>
</ul>
</li>
<li>Solution 1: Always fetch pages from the server<ul>
<li>Poor access time if there is no local copy </li>
</ul>
</li>
<li>Solution (2): Allow Web server to invalidate or update each cached copy<ul>
<li>Degrade 降低 the overall performance<h2 id="Replication-for-Performance"><a href="#Replication-for-Performance" class="headerlink" title="Replication for Performance"></a>Replication for Performance</h2></li>
</ul>
</li>
<li><code>Local data replication</code> helps to reduce <code>access time</code> and solve <code>scalability</code> problems. </li>
<li>However, the following problems arise from data replication:<ul>
<li>Keeping multiple copies <code>up to date</code> may require more <code>network bandwidth</code></li>
<li>Keeping multiple copies consistent may itself be subject to <code>serious scalability problems</code>.<br>Intuitively, a collection of copies is consistent when the copies are always the same.<h2 id="Tight-Consistency-through-Synchronous-Replication"><a href="#Tight-Consistency-through-Synchronous-Replication" class="headerlink" title="Tight Consistency through Synchronous Replication"></a>Tight Consistency through Synchronous Replication</h2></li>
</ul>
</li>
<li>To guarantee <code>tight consistency</code>, a data <code>update</code> should be performed to <code>all data copies</code> at the same time</li>
<li>In other words, an update must performed at all copies as a <code>single atomic operation</code>. </li>
<li>Implementation of such atomic operation, involving potentially a large number replicas, is inherently 天生地 difficult:<ul>
<li>The replicas may be widely dispersed across a <code>large-scale network</code>.</li>
<li>Operations on the replicas may be required to complete <code>quickly</code>.<h2 id="Relaxing-Tight-Consistency"><a href="#Relaxing-Tight-Consistency" class="headerlink" title="Relaxing Tight Consistency"></a>Relaxing Tight Consistency</h2></li>
</ul>
</li>
<li>Global synchronization takes a lot of communication time, especially when replicas are spread across a wide-area network.<ul>
<li>Solution: <code>relax the consistency constraints</code>.</li>
</ul>
</li>
<li>The <code>(instantaneous 瞬间的 ) global synchronizations are avoided</code> if consistency constraints are relaxed, performance may be improved.<ul>
<li>Cons: replicas may not always be the same everywhere.</li>
</ul>
</li>
<li>The extent 程度 to which consistency can be relaxed highly depends on the following:<ul>
<li>the access and update <code>patterns </code>of the replicated data,</li>
<li>the <code>purpose</code> for which the data are used.<h2 id="Consistency-Models-Assumptions"><a href="#Consistency-Models-Assumptions" class="headerlink" title="Consistency Models - Assumptions"></a>Consistency Models - Assumptions</h2></li>
</ul>
</li>
<li>Assuming that there is a data store to which multiple processes running on different machines have access, a consistency model is a <code>“contract” between the processes and this data store.</code><ul>
<li>The store “promises” to work correctly provided that the defined rules are obeyed by the processes.</li>
</ul>
</li>
<li>Local write operations (performed on copy of the data) are propagated 传播 to the other copies.</li>
<li>A data operation is classified as a write operation when it <code>changes the data</code>, otherwise, is a read operation.<br>• Any consistency model <code>restricts</code> the values that <code>a read operation on a data item can return</code>.<h2 id="Sequential-Consistency-Model"><a href="#Sequential-Consistency-Model" class="headerlink" title="Sequential Consistency Model"></a>Sequential Consistency Model</h2></li>
<li>A data store is sequentially consistent when it satisfies the condition:<ul>
<li>“The result of any execution is <code>the same as</code> if the (read and write) operations by all processes on the data store were executed <code>in some sequential order</code> and the operations of each individual process appear in this sequence in the order specified by its program.”</li>
</ul>
</li>
<li>There is no involvement of time; <code>no reference to the “most recent” write operation.</code></li>
</ul>
<p>Example 1</p>
<ul>
<li>Consider four processes operating on the same data item x.<ul>
<li>Process <code>P1</code> first performs a write operation <code>W1(x)a</code> on x by setting the value of x to a.</li>
<li>Later, process P2 also performs a write operation <code>W2(x)b</code>, by setting the value of x to b .</li>
</ul>
</li>
<li>Although, both processes P3 and P4 first read value b, and later value a (i.e., the write operation W2(x)b of process P2 appears to have taken place <code>before W1(x)a of P1</code>), the figure shows sequentially consistent store.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P1: W(x)a</span><br><span class="line">P2:        W(x)b</span><br><span class="line">P3:              R(x)b     R(x)a</span><br><span class="line">P4:                 R(x)b  R(x)a</span><br></pre></td></tr></table></figure>
Example 2</li>
<li>In contrast, the scenario shown bellow violates sequential consistency because <code>not all processes see the same interleaving of write operations</code>.<ul>
<li> P3 will have ‘a’ as final value of x whereas P4 will conclude that the final value is ‘b’.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P1: W(x)a</span><br><span class="line">P2:        W(x)b</span><br><span class="line">P3:              R(x)b     R(x)a</span><br><span class="line">P4:                 R(x)a  R(x)b</span><br></pre></td></tr></table></figure>
<h2 id="Causal-Consistency-Model"><a href="#Causal-Consistency-Model" class="headerlink" title="Causal Consistency Model"></a>Causal Consistency Model</h2></li>
</ul>
</li>
<li>A data store is causally consistent if it satisfies the condition:<ul>
<li>“Writes that are potentially causally related must be seen<code> by all processes in the same order</code>. Concurrent writes may be seen in a <code>different order</code> on <code>different machines</code>.”</li>
</ul>
</li>
<li>This model distinguishes between events that are possibly causally related and those that are not.<ul>
<li>For example, if event b is <code>caused or influenced</code> by an earlier event a, causality requires that<code> “everyone else” first sees a, then see b</code>.</li>
</ul>
</li>
<li>Operations that are <code>not causally related</code> are said to be <code>concurrent</code></li>
</ul>
<p>Example 1</p>
<ul>
<li>The following event sequence is allowed with causally consistent store only (i.e., it is not a sequentially consistent store).</li>
<li>Note that the writes W2(x)b and W1(x)c are <code>concurrent,</code> so it is <code>not required</code> that all processes see them in the same order.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P1: W(x)a                          W(x)c</span><br><span class="line">P2:        R(x)a    W(x)b</span><br><span class="line">P3:            R(x)a                   R(x)c      R(x)b</span><br><span class="line">P4:       R(x)a                         R(x)b    R(x)c</span><br></pre></td></tr></table></figure>
Example 2</li>
<li>Consider the following event sequence where <code>W2(x)b</code> potentially <code>depends </code>on <code>W1(x)a</code> because writing the value b into x <code>may be a result of a computation involving the previously read value by R2(x)a</code>.</li>
<li>Note that the two writes are <code>causally related</code>, so all processes must see them in <code>the same order</code>. Therefore, the event sequence is <code>incorrect</code>.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P1: W(x)a                     </span><br><span class="line">P2:        R(x)a    W(x)b</span><br><span class="line">P3:                           R(x)b     R(x)a</span><br><span class="line">P4:                             R(x)a   R(x)b   </span><br></pre></td></tr></table></figure>
Example 3</li>
<li>Consider the following event sequence where W1(x)a and W2(x)b are concurrent writes.</li>
<li>As a causally consistent store does not require concurrent writes to be globally ordered, this event sequence is correct. Note, however, that it reflects a situation that would not be acceptable for a sequentially consistent store.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P1: W(x)a                     </span><br><span class="line">P2:                   W(x)b</span><br><span class="line">P3:                           R(x)b     R(x)a</span><br><span class="line">P4:                             R(x)a   R(x)b   </span><br></pre></td></tr></table></figure>
Eventual consistency: given a <code>sufficiently long period of time</code> over which no changes are sent, all updates are expected to propagate, and eventually <code>all replicas will be consistent</code>.<h2 id="Replica-Management"><a href="#Replica-Management" class="headerlink" title="Replica Management"></a>Replica Management</h2></li>
<li>A Greedy Heuristic to Find Locations (minimum k-median problem) <ol>
<li>Find the <code>total cost</code> of accessing each site from all the other sites. Choose the site with the<code> minimum total cost</code>. </li>
<li> Repeat (1) above, taking also into account sites hosting replicas (i.e , recalculate costs).</li>
</ol>
</li>
</ul>
<h1 id="Security-in-Distributed-Systems"><a href="#Security-in-Distributed-Systems" class="headerlink" title="Security in Distributed Systems"></a>Security in Distributed Systems</h1><ul>
<li>Security in distributed systems can roughly be divided into two parts:<ol>
<li>Communication between users or processes, possibly residing 位于 on <em>different machines</em>. <ul>
<li>Use <em>secure channel</em> to deal.</li>
</ul>
</li>
<li>Authorization, to ensure that a process gets only those access rights to the resources in a distributed system it is entitled 有资格 to.<ul>
<li>Access control mechanisms<h2 id="Relationship-between-Security-and-Dependability"><a href="#Relationship-between-Security-and-Dependability" class="headerlink" title="Relationship between Security and Dependability"></a>Relationship between Security and Dependability</h2></li>
</ul>
</li>
</ol>
</li>
<li>Security in a computer system is associated with the notion of dependability, because a dependable system is one that is trusted to deliver its services.</li>
<li>Recall that Dependability is about availability, reliability, safety, and maintainability<ul>
<li>Which indirectly suggest <em>TRUST</em> (i.e., also include confidentiality and integrity 保密性和完整性). </li>
</ul>
</li>
<li>Confidentiality refers to the property of a computer system whereby its information is disclosed 披露 only to authorized parties.</li>
<li>Integrity is the characteristic that alterations 更改 to a system’s assets can be made only in an authorized way.<h2 id="Types-of-Security-Threats"><a href="#Types-of-Security-Threats" class="headerlink" title="Types of Security Threats"></a>Types of Security Threats</h2></li>
<li>Security in computer systems involves mechanisms to protect system services and data against security threats. </li>
<li>Four types of security threats:<ul>
<li>Interception: 拦截 an unauthorized party has gained access to a service or data.<ul>
<li>Example: When communication between two parties has been overheard by someone else</li>
</ul>
</li>
<li>Interruption: services or data become unavailable, unusable, destroyed, and so on.<ul>
<li>Examples: denial of service attacks by which someone maliciously attempts to make a service inaccessible to other parties; when a file is corrupted 损坏 or lost, etc.</li>
</ul>
</li>
<li>Modification: unauthorized changing of data or tampering 篡改 with a service<ul>
<li>Examples: intercepting and subsequently changing transmitted data, tampering with database entries, etc.</li>
</ul>
</li>
<li>Fabrication 制造: additional data or activity are generated that would normally not exist.<ul>
<li>Examples: an intruder 入侵者 may attempt to add an entry into a password file or database; breaking into a system by replaying previously sent messages.<h2 id="Security-Mechanisms"><a href="#Security-Mechanisms" class="headerlink" title="Security Mechanisms"></a>Security Mechanisms</h2></li>
</ul>
</li>
</ul>
</li>
<li>A description of actions that are allowed to be taken and actions that are prohibited to guarantee system security is what we call Security Policy, which impact users, services, data, machines, etc.</li>
<li>Based on a system’s Security Policy, mechanisms can be put in place to enforce 执行 this policy:<ul>
<li>Encryption 加密<ul>
<li>Encryption transforms data into something an attacker cannot understand.</li>
<li>As such, encryption is not only able to implement data confidentiality, but it also supports integrity checks, because it allows checking of whether data has been modified.<ul>
<li>Two types of encryption: symmetric and asymmetric encryption.</li>
<li>In symmetric encryption the <em>same secret value</em> (the key) is used for encryption and decryption whereas different keys are used in asymmetric encryption.</li>
</ul>
</li>
</ul>
</li>
<li>Authentication 验证<ul>
<li>Authentication is used to verify the claimed identity of a user, client, server, host, or other entity.</li>
<li>Authentication is based on the possession 所有权 of some secret information, like password, known only to the entities participating in the authentication.</li>
<li>When an entity wants to authenticate another entity, the former will verify if the latter possesses the knowledge of the secret</li>
<li>Authentication can be one-way or mutual.<ul>
<li>In one-way authentication, only one entity verifies the identity of the other entity. </li>
<li>In mutual authentication, both communicating entities verify each other’s identity.</li>
</ul>
</li>
</ul>
</li>
<li>Authorization <ul>
<li>Authorization checks whether a client is authorized to perform the action requested.<ul>
<li>Example: For accessing records in a medical database, such that depending on who accesses the database, permission may be granted to read records, to modify certain fields in a record.</li>
</ul>
</li>
</ul>
</li>
<li>Auditing 审计<ul>
<li>Auditing tools are used to trace which clients accessed what, and in which way. </li>
<li> Audit <em>logs</em> can be extremely useful for the analysis of a security breach 安全漏洞, and subsequently taking measures against intruders.<h2 id="Cryptography-密码学"><a href="#Cryptography-密码学" class="headerlink" title="Cryptography 密码学"></a>Cryptography 密码学</h2></li>
</ul>
</li>
</ul>
</li>
<li>Cryptography is a fundamental security technique in distributed systems</li>
<li>Consider a sender S wanting to transmit message m to a receiver R. To protect the message against security threats, the sender <ol>
<li> encrypts it into an unintelligible message m’, </li>
<li> sends m’ to R</li>
<li> R, in turn, must decrypt the received message into its original form m.</li>
</ol>
</li>
<li>The original form of the message that is sent is called the <code>plaintext</code>, shown as <code>P</code>; the encrypted form is referred to as the <code>ciphertext</code>, illustrated as <code>C</code>.<h2 id="Security-in-Distributed-System-Recap"><a href="#Security-in-Distributed-System-Recap" class="headerlink" title="Security in Distributed System-Recap"></a>Security in Distributed System-Recap</h2></li>
<li>Two main issues that need to be addressed: </li>
</ul>
<ol>
<li>Making the <code>communication</code> between clients and servers secure.<ul>
<li>This may involve the following: authentication of the communicating parties as well as integrity and confidentiality of messages.</li>
</ul>
</li>
<li>Controlling <code>access</code> to resources.<ul>
<li>Once a server has accepted a request from a client, how can it find out whether that client is <code>authorized</code> to have that request carried out?<h2 id="Secure-channels"><a href="#Secure-channels" class="headerlink" title="Secure channels"></a>Secure channels</h2><blockquote>
<p>“The issue of protecting communication between clients and servers, can be thought of in terms of setting up a secure channel between communicating parties.”</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<ul>
<li>A secure channel protects senders and receivers against interception, modification, and fabrication of messages.</li>
<li>Protecting messages against interception is done by ensuring confidentiality: the secure channel ensures that its messages cannot be eavesdropped 窃听 by intruders.</li>
<li>Protecting against modification and fabrication by intruders is done through <code>protocols</code> for mutual 相互的 authentication and message integrity.<h2 id="Example-of-Autentication-Protocol"><a href="#Example-of-Autentication-Protocol" class="headerlink" title="Example of Autentication Protocol"></a>Example of Autentication Protocol</h2></li>
<li>Consider that Alice and Bob want to communicate, and that Alice takes the initiative 倡议 in setting up a channel.</li>
<li>Alice starts by sending a message to Bob, or otherwise to a trusted third party who will help set up the channel.</li>
<li>Once the channel has been set up, Alice knows for sure that she is talking to Bob, and Bob knows for sure he is talking to Alice, they can exchange messages.<h2 id="Secure-channels-Message-integrity-and-confidentiality"><a href="#Secure-channels-Message-integrity-and-confidentiality" class="headerlink" title="Secure channels: Message integrity and confidentiality"></a>Secure channels: Message integrity and confidentiality</h2></li>
<li>Besides authentication, a secure channel should also provide <code>guarantees</code> for <code>message integrity and confidentiality</code> </li>
<li>Confidentiality is established by simply <code>encrypting</code>  a message before sending it. </li>
<li>Message integrity can be obtained via the use of <code>digital signatures</code>.<h2 id="Message-integrity-via-Digital-Signature"><a href="#Message-integrity-via-Digital-Signature" class="headerlink" title="Message integrity via Digital Signature"></a>Message integrity via Digital Signature</h2></li>
<li>Consider the situation in which Bob has just sold Alice a collector’s item of some vinyl record 黑胶唱片 for $500. The whole deal was done through e-mail. </li>
<li>Besides authentication, at least two concerns that need to be addressed:<ol>
<li>Alice needs to be assured that Bob will not maliciously change the $500 and claim she promised more than $500. </li>
<li>Bob needs to be assured that Alice cannot deny ever having sent the message.</li>
</ol>
</li>
<li>Possible solution: Alice <code>digitally signs the message</code>, uniquely binding her signature to its content.<ul>
<li> The unique association between a message and its signature prevents <code>illegitimate modifications</code> and <code>backing out from the agreement</code>.</li>
</ul>
</li>
<li>One common form of digital signature is to use a <code>public-key cryptosystem</code>.</li>
<li>Subject -(Request for operation)-&gt; <code>Reference monitor</code> -(Authorized request)-&gt; <code>Object</code></li>
<li>Controlling the access to an object means <code>protecting it against</code> requests<br>generated by <code>unauthorized</code> subjects</li>
<li>Protection is often <code>enforced</code> by a program called a <code>reference monitor</code><ul>
<li>A reference monitor records which subject may do what, and <code>decides</code> whether a subject <code>is allowed</code> to have a specific operation carried out.</li>
</ul>
</li>
<li>Reference monitor should be <code>impenetrable</code> 不可穿透的 by its very nature</li>
</ul>
<h1 id="Service-Oriented-Architecture"><a href="#Service-Oriented-Architecture" class="headerlink" title="Service-Oriented Architecture"></a>Service-Oriented Architecture</h1><ul>
<li>This architectural style <em>encapsulates services</em> into independent units. </li>
<li>A service is considered as a discrete <em>unit of functionality</em> that can be <em>accessed remotely</em>, via a network, and updated independently. However, a service can possibly make use of other services.<ul>
<li>Example: a procedure to retrieve a credit card statement online.  </li>
</ul>
</li>
<li>Distributed systems or distributed applications constructed using this architectural style are said to be have a Service-Oriented Architecture (SOA)</li>
<li>A distributed system constructed as SOA is a essentially a composition of many different services.<h2 id="Example-of-a-SOA-Application"><a href="#Example-of-a-SOA-Application" class="headerlink" title="Example of a SOA Application"></a>Example of a SOA Application</h2></li>
<li>Consider a distributed system application composed of several services for processing e-book orders from a Web Shop.<ul>
<li>Note that not all services composing a distributed system application may belong to the same administrative organization.</li>
<li>On-line order processing:<ul>
<li>Organisation A: select items -&gt; check delivery channel -&gt;</li>
<li>Organisation B: process payment -&gt;</li>
<li>Organisation A: Finish the payment and do other things<h2 id="Service-Composition"><a href="#Service-Composition" class="headerlink" title="Service Composition"></a>Service Composition</h2></li>
</ul>
</li>
</ul>
</li>
<li>One of the main challenges of developing a distributed system is <em>service composition</em> and of making sure that the services operate in harmony. 和谐</li>
<li>For service composition to be possible, each service must offer an interface (including the allowed input and output messages). </li>
<li>Service composition is a far from trivial problem. 不是一个微不足道的问题</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Distributed-System/" rel="tag"># Distributed System</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/04/22/Programming-Languages-and-Paradigms-Part-1/" rel="prev" title="Programming Languages and Paradigms -- Part 1">
                  <i class="fa fa-chevron-left"></i> Programming Languages and Paradigms -- Part 1
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yangyang Cui</span>
</div>
<div class="busuanzi-count">
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
